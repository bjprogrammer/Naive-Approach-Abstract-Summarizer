{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reviews.csv contains amazon fine foods reviews\n",
    "reviews = pd.read_csv(\"Reviews.csv\")\n",
    "# Remove null values and unneeded features\n",
    "reviews = reviews.dropna()\n",
    "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n",
    "                        'Score','Time'], 1)\n",
    "reviews = reviews.reset_index(drop=True)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        # We are not using \"text.split()\" here\n",
    "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
    "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 1917248\n"
     ]
    }
   ],
   "source": [
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('C:/Users/bobby/Downloads/AmazonReviews-Summarization/numberbatch.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 125885\n",
      "Number of words missing from CN: 22171\n",
      "Percent of words that are missing from vocabulary: 17.61%\n"
     ]
    }
   ],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1\n",
    "                \n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))\n",
    "\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 125885\n",
      "Number of words we will use: 22701\n",
      "Percent of words we will use: 18.029999999999998%\n"
     ]
    }
   ],
   "source": [
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22701\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 49652961\n",
      "Total number of UNKs in headlines: 338764\n",
      "Percent of words that are UNK: 0.6799999999999999%\n"
     ]
    }
   ],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count\n",
    "\n",
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "              counts\n",
      "count  568411.000000\n",
      "mean        4.181212\n",
      "std         2.657213\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         5.000000\n",
      "max        48.000000\n",
      "\n",
      "Texts:\n",
      "              counts\n",
      "count  568411.000000\n",
      "mean       84.172764\n",
      "std        83.095213\n",
      "min         4.000000\n",
      "25%        35.000000\n",
      "50%        59.000000\n",
      "75%       102.000000\n",
      "max      3530.000000\n"
     ]
    }
   ],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])\n",
    "\n",
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304811\n",
      "304811\n"
     ]
    }
   ],
   "source": [
    "max_text_length = 83 # This will cover up to 89.5% lengthes\n",
    "max_summary_length = 13 # This will cover up to 99% lengthes\n",
    "min_length = 2\n",
    "unk_text_limit = 1 # text can contain up to 1 UNK word\n",
    "unk_summary_limit = 0 # Summary should not contain any UNK word\n",
    "\n",
    "def filter_condition(item):\n",
    "    int_summary = item[0]\n",
    "    int_text = item[1]\n",
    "    if(len(int_summary) >= min_length and \n",
    "       len(int_summary) <= max_summary_length and \n",
    "       len(int_text) >= min_length and \n",
    "       len(int_text) <= max_text_length and \n",
    "       unk_counter(int_summary) <= unk_summary_limit and \n",
    "       unk_counter(int_text) <= unk_text_limit):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "int_text_summaries = list(zip(int_summaries , int_texts))\n",
    "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\n",
    "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\n",
    "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\n",
    "sorted_summaries = list(sorted_int_text_summaries[0])\n",
    "sorted_texts = list(sorted_int_text_summaries[1])\n",
    "# Delete those temporary varaibles\n",
    "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "__pickleStuff(\"./data/clean_summaries.p\",clean_summaries)\n",
    "__pickleStuff(\"./data/clean_texts.p\",clean_texts)\n",
    "\n",
    "__pickleStuff(\"./data/sorted_summaries.p\",sorted_summaries)\n",
    "__pickleStuff(\"./data/sorted_texts.p\",sorted_texts)\n",
    "__pickleStuff(\"./data/word_embedding_matrix.p\",word_embedding_matrix)\n",
    "\n",
    "__pickleStuff(\"./data/vocab_to_int.p\",vocab_to_int)\n",
    "__pickleStuff(\"./data/int_to_vocab.p\",int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):  \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    layer_input = rnn_inputs\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    layer_input,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "            layer_input = tf.concat(enc_output, 2)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n",
    "                            vocab_size, max_summary_length,batch_size):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                       helper=training_helper,\n",
    "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                       output_layer = output_layer)\n",
    "\n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                     enc_output,\n",
    "                                                     text_length,\n",
    "                                                     normalize=False,\n",
    "                                                     name='BahdanauAttention')\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size,\n",
    "                                                  max_summary_length,\n",
    "                                                  batch_size)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,\n",
    "                                                    vocab_to_int['<GO>'],\n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell,\n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<PAD>' has id: 22698\n",
      "pad summaries batch samples:\n",
      "\r",
      " [[  200   225   174   373 22698 22698 22698 22698 22698 22698 22698 22698\n",
      "  22698]\n",
      " [ 3392    89    67   890    67  1410   431   159   317   231 22698 22698\n",
      "  22698]\n",
      " [ 5534  5535   557   558   599   483  2383  1911   672   109    41   330\n",
      "  22698]\n",
      " [  181  1112   557   558   867  3017  1971  2383  1911   672   109    41\n",
      "    330]\n",
      " [ 3392    89    67   890    67  1410   431   159   317   231 22698 22698\n",
      "  22698]]\n"
     ]
    }
   ],
   "source": [
    "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n",
    "sorted_summaries_samples = sorted_summaries[7:50]\n",
    "sorted_texts_samples = sorted_texts[7:50]\n",
    "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n",
    "    sorted_summaries_samples, sorted_texts_samples, 5))\n",
    "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 4\n",
      "The longest text length: 20\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 200000\n",
    "end = start + 10000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 Batch   20/156 - Loss:  5.478, Seconds: 7.64\n",
      "Epoch   1/100 Batch   40/156 - Loss:  2.506, Seconds: 7.75\n",
      "Average loss for this update: 3.684\n",
      "New Record!\n",
      "Epoch   1/100 Batch   60/156 - Loss:  2.551, Seconds: 9.22\n",
      "Epoch   1/100 Batch   80/156 - Loss:  2.265, Seconds: 7.78\n",
      "Epoch   1/100 Batch  100/156 - Loss:  2.326, Seconds: 10.13\n",
      "Average loss for this update: 2.325\n",
      "New Record!\n",
      "Epoch   1/100 Batch  120/156 - Loss:  2.270, Seconds: 10.20\n",
      "Epoch   1/100 Batch  140/156 - Loss:  2.232, Seconds: 9.99\n",
      "Average loss for this update: 2.237\n",
      "New Record!\n",
      "Epoch   2/100 Batch   20/156 - Loss:  2.694, Seconds: 7.64\n",
      "Epoch   2/100 Batch   40/156 - Loss:  2.026, Seconds: 7.96\n",
      "Average loss for this update: 2.295\n",
      "No Improvement.\n",
      "Epoch   2/100 Batch   60/156 - Loss:  2.071, Seconds: 8.46\n",
      "Epoch   2/100 Batch   80/156 - Loss:  1.777, Seconds: 7.60\n",
      "Epoch   2/100 Batch  100/156 - Loss:  1.898, Seconds: 9.77\n",
      "Average loss for this update: 1.873\n",
      "New Record!\n",
      "Epoch   2/100 Batch  120/156 - Loss:  1.799, Seconds: 10.69\n",
      "Epoch   2/100 Batch  140/156 - Loss:  1.774, Seconds: 10.69\n",
      "Average loss for this update: 1.764\n",
      "New Record!\n",
      "Epoch   3/100 Batch   20/156 - Loss:  2.273, Seconds: 8.24\n",
      "Epoch   3/100 Batch   40/156 - Loss:  1.741, Seconds: 7.91\n",
      "Average loss for this update: 1.942\n",
      "No Improvement.\n",
      "Epoch   3/100 Batch   60/156 - Loss:  1.715, Seconds: 9.24\n",
      "Epoch   3/100 Batch   80/156 - Loss:  1.444, Seconds: 8.08\n",
      "Epoch   3/100 Batch  100/156 - Loss:  1.570, Seconds: 9.59\n",
      "Average loss for this update: 1.54\n",
      "New Record!\n",
      "Epoch   3/100 Batch  120/156 - Loss:  1.436, Seconds: 10.16\n",
      "Epoch   3/100 Batch  140/156 - Loss:  1.413, Seconds: 10.23\n",
      "Average loss for this update: 1.409\n",
      "New Record!\n",
      "Epoch   4/100 Batch   20/156 - Loss:  1.885, Seconds: 7.56\n",
      "Epoch   4/100 Batch   40/156 - Loss:  1.481, Seconds: 7.86\n",
      "Average loss for this update: 1.624\n",
      "No Improvement.\n",
      "Epoch   4/100 Batch   60/156 - Loss:  1.421, Seconds: 8.54\n",
      "Epoch   4/100 Batch   80/156 - Loss:  1.209, Seconds: 7.86\n",
      "Epoch   4/100 Batch  100/156 - Loss:  1.300, Seconds: 9.69\n",
      "Average loss for this update: 1.282\n",
      "New Record!\n",
      "Epoch   4/100 Batch  120/156 - Loss:  1.258, Seconds: 9.92\n",
      "Epoch   4/100 Batch  140/156 - Loss:  1.226, Seconds: 11.34\n",
      "Average loss for this update: 1.227\n",
      "New Record!\n",
      "Epoch   5/100 Batch   20/156 - Loss:  1.593, Seconds: 8.39\n",
      "Epoch   5/100 Batch   40/156 - Loss:  1.362, Seconds: 7.76\n",
      "Average loss for this update: 1.496\n",
      "No Improvement.\n",
      "Epoch   5/100 Batch   60/156 - Loss:  1.508, Seconds: 9.07\n",
      "Epoch   5/100 Batch   80/156 - Loss:  1.178, Seconds: 8.00\n",
      "Epoch   5/100 Batch  100/156 - Loss:  1.277, Seconds: 9.57\n",
      "Average loss for this update: 1.262\n",
      "No Improvement.\n",
      "Epoch   5/100 Batch  120/156 - Loss:  1.178, Seconds: 10.24\n",
      "Epoch   5/100 Batch  140/156 - Loss:  1.136, Seconds: 10.45\n",
      "Average loss for this update: 1.149\n",
      "New Record!\n",
      "Epoch   6/100 Batch   20/156 - Loss:  1.456, Seconds: 7.78\n",
      "Epoch   6/100 Batch   40/156 - Loss:  1.196, Seconds: 8.36\n",
      "Average loss for this update: 1.289\n",
      "No Improvement.\n",
      "Epoch   6/100 Batch   60/156 - Loss:  1.163, Seconds: 8.72\n",
      "Epoch   6/100 Batch   80/156 - Loss:  0.958, Seconds: 7.68\n",
      "Epoch   6/100 Batch  100/156 - Loss:  0.995, Seconds: 9.55\n",
      "Average loss for this update: 1.009\n",
      "New Record!\n",
      "Epoch   6/100 Batch  120/156 - Loss:  0.937, Seconds: 10.31\n",
      "Epoch   6/100 Batch  140/156 - Loss:  0.896, Seconds: 10.67\n",
      "Average loss for this update: 0.905\n",
      "New Record!\n",
      "Epoch   7/100 Batch   20/156 - Loss:  1.142, Seconds: 7.66\n",
      "Epoch   7/100 Batch   40/156 - Loss:  0.981, Seconds: 7.82\n",
      "Average loss for this update: 1.034\n",
      "No Improvement.\n",
      "Epoch   7/100 Batch   60/156 - Loss:  0.928, Seconds: 8.69\n",
      "Epoch   7/100 Batch   80/156 - Loss:  0.781, Seconds: 7.24\n",
      "Epoch   7/100 Batch  100/156 - Loss:  0.816, Seconds: 9.36\n",
      "Average loss for this update: 0.818\n",
      "New Record!\n",
      "Epoch   7/100 Batch  120/156 - Loss:  0.794, Seconds: 10.33\n",
      "Epoch   7/100 Batch  140/156 - Loss:  0.761, Seconds: 10.07\n",
      "Average loss for this update: 0.768\n",
      "New Record!\n",
      "Epoch   8/100 Batch   20/156 - Loss:  0.967, Seconds: 8.76\n",
      "Epoch   8/100 Batch   40/156 - Loss:  0.834, Seconds: 7.89\n",
      "Average loss for this update: 0.881\n",
      "No Improvement.\n",
      "Epoch   8/100 Batch   60/156 - Loss:  0.812, Seconds: 8.96\n",
      "Epoch   8/100 Batch   80/156 - Loss:  0.687, Seconds: 7.59\n",
      "Epoch   8/100 Batch  100/156 - Loss:  0.708, Seconds: 9.30\n",
      "Average loss for this update: 0.717\n",
      "New Record!\n",
      "Epoch   8/100 Batch  120/156 - Loss:  0.703, Seconds: 10.07\n",
      "Epoch   8/100 Batch  140/156 - Loss:  0.721, Seconds: 10.17\n",
      "Average loss for this update: 0.705\n",
      "New Record!\n",
      "Epoch   9/100 Batch   20/156 - Loss:  0.871, Seconds: 7.60\n",
      "Epoch   9/100 Batch   40/156 - Loss:  0.764, Seconds: 8.48\n",
      "Average loss for this update: 0.804\n",
      "No Improvement.\n",
      "Epoch   9/100 Batch   60/156 - Loss:  0.755, Seconds: 8.58\n",
      "Epoch   9/100 Batch   80/156 - Loss:  0.623, Seconds: 8.52\n",
      "Epoch   9/100 Batch  100/156 - Loss:  0.665, Seconds: 9.71\n",
      "Average loss for this update: 0.662\n",
      "New Record!\n",
      "Epoch   9/100 Batch  120/156 - Loss:  0.637, Seconds: 11.51\n",
      "Epoch   9/100 Batch  140/156 - Loss:  0.634, Seconds: 11.17\n",
      "Average loss for this update: 0.636\n",
      "New Record!\n",
      "Epoch  10/100 Batch   20/156 - Loss:  0.724, Seconds: 9.51\n",
      "Epoch  10/100 Batch   40/156 - Loss:  0.653, Seconds: 7.98\n",
      "Average loss for this update: 0.676\n",
      "No Improvement.\n",
      "Epoch  10/100 Batch   60/156 - Loss:  0.629, Seconds: 8.58\n",
      "Epoch  10/100 Batch   80/156 - Loss:  0.528, Seconds: 7.80\n",
      "Epoch  10/100 Batch  100/156 - Loss:  0.562, Seconds: 9.45\n",
      "Average loss for this update: 0.558\n",
      "New Record!\n",
      "Epoch  10/100 Batch  120/156 - Loss:  0.534, Seconds: 10.07\n",
      "Epoch  10/100 Batch  140/156 - Loss:  0.549, Seconds: 10.17\n",
      "Average loss for this update: 0.54\n",
      "New Record!\n",
      "Epoch  11/100 Batch   20/156 - Loss:  0.614, Seconds: 7.68\n",
      "Epoch  11/100 Batch   40/156 - Loss:  0.568, Seconds: 7.90\n",
      "Average loss for this update: 0.58\n",
      "No Improvement.\n",
      "Epoch  11/100 Batch   60/156 - Loss:  0.557, Seconds: 9.06\n",
      "Epoch  11/100 Batch   80/156 - Loss:  0.471, Seconds: 7.72\n",
      "Epoch  11/100 Batch  100/156 - Loss:  0.510, Seconds: 9.55\n",
      "Average loss for this update: 0.507\n",
      "New Record!\n",
      "Epoch  11/100 Batch  120/156 - Loss:  0.493, Seconds: 10.30\n",
      "Epoch  11/100 Batch  140/156 - Loss:  0.490, Seconds: 10.19\n",
      "Average loss for this update: 0.489\n",
      "New Record!\n",
      "Epoch  12/100 Batch   20/156 - Loss:  0.584, Seconds: 7.66\n",
      "Epoch  12/100 Batch   40/156 - Loss:  0.514, Seconds: 7.80\n",
      "Average loss for this update: 0.544\n",
      "No Improvement.\n",
      "Epoch  12/100 Batch   60/156 - Loss:  0.533, Seconds: 9.32\n",
      "Epoch  12/100 Batch   80/156 - Loss:  0.452, Seconds: 7.62\n",
      "Epoch  12/100 Batch  100/156 - Loss:  0.487, Seconds: 9.51\n",
      "Average loss for this update: 0.484\n",
      "New Record!\n",
      "Epoch  12/100 Batch  120/156 - Loss:  0.480, Seconds: 10.53\n",
      "Epoch  12/100 Batch  140/156 - Loss:  0.461, Seconds: 11.45\n",
      "Average loss for this update: 0.464\n",
      "New Record!\n",
      "Epoch  13/100 Batch   20/156 - Loss:  0.563, Seconds: 7.62\n",
      "Epoch  13/100 Batch   40/156 - Loss:  0.496, Seconds: 7.82\n",
      "Average loss for this update: 0.516\n",
      "No Improvement.\n",
      "Epoch  13/100 Batch   60/156 - Loss:  0.476, Seconds: 9.59\n",
      "Epoch  13/100 Batch   80/156 - Loss:  0.404, Seconds: 7.77\n",
      "Epoch  13/100 Batch  100/156 - Loss:  0.452, Seconds: 10.46\n",
      "Average loss for this update: 0.439\n",
      "New Record!\n",
      "Epoch  13/100 Batch  120/156 - Loss:  0.434, Seconds: 10.17\n",
      "Epoch  13/100 Batch  140/156 - Loss:  0.417, Seconds: 11.01\n",
      "Average loss for this update: 0.42\n",
      "New Record!\n",
      "Epoch  14/100 Batch   20/156 - Loss:  0.462, Seconds: 7.68\n",
      "Epoch  14/100 Batch   40/156 - Loss:  0.421, Seconds: 8.10\n",
      "Average loss for this update: 0.433\n",
      "No Improvement.\n",
      "Epoch  14/100 Batch   60/156 - Loss:  0.408, Seconds: 8.64\n",
      "Epoch  14/100 Batch   80/156 - Loss:  0.369, Seconds: 7.82\n",
      "Epoch  14/100 Batch  100/156 - Loss:  0.385, Seconds: 9.36\n",
      "Average loss for this update: 0.384\n",
      "New Record!\n",
      "Epoch  14/100 Batch  120/156 - Loss:  0.369, Seconds: 11.29\n",
      "Epoch  14/100 Batch  140/156 - Loss:  0.367, Seconds: 10.13\n",
      "Average loss for this update: 0.363\n",
      "New Record!\n",
      "Epoch  15/100 Batch   20/156 - Loss:  0.435, Seconds: 7.82\n",
      "Epoch  15/100 Batch   40/156 - Loss:  0.377, Seconds: 7.84\n",
      "Average loss for this update: 0.395\n",
      "No Improvement.\n",
      "Epoch  15/100 Batch   60/156 - Loss:  0.363, Seconds: 8.74\n",
      "Epoch  15/100 Batch   80/156 - Loss:  0.320, Seconds: 8.18\n",
      "Epoch  15/100 Batch  100/156 - Loss:  0.357, Seconds: 9.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for this update: 0.345\n",
      "New Record!\n",
      "Epoch  15/100 Batch  120/156 - Loss:  0.337, Seconds: 10.09\n",
      "Epoch  15/100 Batch  140/156 - Loss:  0.317, Seconds: 10.19\n",
      "Average loss for this update: 0.324\n",
      "New Record!\n",
      "Epoch  16/100 Batch   20/156 - Loss:  0.368, Seconds: 7.60\n",
      "Epoch  16/100 Batch   40/156 - Loss:  0.361, Seconds: 7.76\n",
      "Average loss for this update: 0.359\n",
      "No Improvement.\n",
      "Epoch  16/100 Batch   60/156 - Loss:  0.354, Seconds: 8.62\n",
      "Epoch  16/100 Batch   80/156 - Loss:  0.304, Seconds: 7.65\n",
      "Epoch  16/100 Batch  100/156 - Loss:  0.341, Seconds: 9.39\n",
      "Average loss for this update: 0.332\n",
      "No Improvement.\n",
      "Epoch  16/100 Batch  120/156 - Loss:  0.319, Seconds: 10.09\n",
      "Epoch  16/100 Batch  140/156 - Loss:  0.302, Seconds: 11.06\n",
      "Average loss for this update: 0.312\n",
      "New Record!\n",
      "Epoch  17/100 Batch   20/156 - Loss:  0.368, Seconds: 7.70\n",
      "Epoch  17/100 Batch   40/156 - Loss:  0.353, Seconds: 8.16\n",
      "Average loss for this update: 0.35\n",
      "No Improvement.\n",
      "Epoch  17/100 Batch   60/156 - Loss:  0.320, Seconds: 8.72\n",
      "Epoch  17/100 Batch   80/156 - Loss:  0.289, Seconds: 7.74\n",
      "Epoch  17/100 Batch  100/156 - Loss:  0.329, Seconds: 10.87\n",
      "Average loss for this update: 0.313\n",
      "No Improvement.\n",
      "Epoch  17/100 Batch  120/156 - Loss:  0.321, Seconds: 11.07\n",
      "Epoch  17/100 Batch  140/156 - Loss:  0.318, Seconds: 11.75\n",
      "Average loss for this update: 0.318\n",
      "No Improvement.\n",
      "Epoch  18/100 Batch   20/156 - Loss:  0.362, Seconds: 7.70\n",
      "Epoch  18/100 Batch   40/156 - Loss:  0.332, Seconds: 8.36\n",
      "Average loss for this update: 0.34\n",
      "No Improvement.\n",
      "Epoch  18/100 Batch   60/156 - Loss:  0.321, Seconds: 10.84\n",
      "Epoch  18/100 Batch   80/156 - Loss:  0.298, Seconds: 7.48\n",
      "Epoch  18/100 Batch  100/156 - Loss:  0.332, Seconds: 10.25\n",
      "Average loss for this update: 0.319\n",
      "No Improvement.\n",
      "Epoch  18/100 Batch  120/156 - Loss:  0.297, Seconds: 11.27\n",
      "Epoch  18/100 Batch  140/156 - Loss:  0.276, Seconds: 10.31\n",
      "Average loss for this update: 0.284\n",
      "New Record!\n",
      "Epoch  19/100 Batch   20/156 - Loss:  0.336, Seconds: 7.66\n",
      "Epoch  19/100 Batch   40/156 - Loss:  0.295, Seconds: 9.36\n",
      "Average loss for this update: 0.304\n",
      "No Improvement.\n",
      "Epoch  19/100 Batch   60/156 - Loss:  0.278, Seconds: 11.27\n",
      "Epoch  19/100 Batch   80/156 - Loss:  0.255, Seconds: 9.06\n",
      "Epoch  19/100 Batch  100/156 - Loss:  0.277, Seconds: 9.71\n",
      "Average loss for this update: 0.272\n",
      "New Record!\n",
      "Epoch  19/100 Batch  120/156 - Loss:  0.278, Seconds: 11.02\n",
      "Epoch  19/100 Batch  140/156 - Loss:  0.263, Seconds: 10.05\n",
      "Average loss for this update: 0.268\n",
      "New Record!\n",
      "Epoch  20/100 Batch   20/156 - Loss:  0.314, Seconds: 7.68\n",
      "Epoch  20/100 Batch   40/156 - Loss:  0.292, Seconds: 7.76\n",
      "Average loss for this update: 0.297\n",
      "No Improvement.\n",
      "Epoch  20/100 Batch   60/156 - Loss:  0.280, Seconds: 8.54\n",
      "Epoch  20/100 Batch   80/156 - Loss:  0.243, Seconds: 7.72\n",
      "Epoch  20/100 Batch  100/156 - Loss:  0.272, Seconds: 9.30\n",
      "Average loss for this update: 0.265\n",
      "New Record!\n",
      "Epoch  20/100 Batch  120/156 - Loss:  0.267, Seconds: 10.09\n",
      "Epoch  20/100 Batch  140/156 - Loss:  0.252, Seconds: 11.23\n",
      "Average loss for this update: 0.254\n",
      "New Record!\n",
      "Epoch  21/100 Batch   20/156 - Loss:  0.299, Seconds: 7.68\n",
      "Epoch  21/100 Batch   40/156 - Loss:  0.258, Seconds: 7.84\n",
      "Average loss for this update: 0.274\n",
      "No Improvement.\n",
      "Epoch  21/100 Batch   60/156 - Loss:  0.261, Seconds: 8.58\n",
      "Epoch  21/100 Batch   80/156 - Loss:  0.248, Seconds: 7.44\n",
      "Epoch  21/100 Batch  100/156 - Loss:  0.257, Seconds: 9.36\n",
      "Average loss for this update: 0.255\n",
      "No Improvement.\n",
      "Epoch  21/100 Batch  120/156 - Loss:  0.245, Seconds: 10.05\n",
      "Epoch  21/100 Batch  140/156 - Loss:  0.241, Seconds: 12.21\n",
      "Average loss for this update: 0.245\n",
      "New Record!\n",
      "Epoch  22/100 Batch   20/156 - Loss:  0.287, Seconds: 7.82\n",
      "Epoch  22/100 Batch   40/156 - Loss:  0.259, Seconds: 8.00\n",
      "Average loss for this update: 0.269\n",
      "No Improvement.\n",
      "Epoch  22/100 Batch   60/156 - Loss:  0.256, Seconds: 8.62\n",
      "Epoch  22/100 Batch   80/156 - Loss:  0.223, Seconds: 7.92\n",
      "Epoch  22/100 Batch  100/156 - Loss:  0.245, Seconds: 9.49\n",
      "Average loss for this update: 0.239\n",
      "New Record!\n",
      "Epoch  22/100 Batch  120/156 - Loss:  0.231, Seconds: 11.00\n",
      "Epoch  22/100 Batch  140/156 - Loss:  0.226, Seconds: 10.35\n",
      "Average loss for this update: 0.234\n",
      "New Record!\n",
      "Epoch  23/100 Batch   20/156 - Loss:  0.271, Seconds: 7.86\n",
      "Epoch  23/100 Batch   40/156 - Loss:  0.238, Seconds: 7.82\n",
      "Average loss for this update: 0.248\n",
      "No Improvement.\n",
      "Epoch  23/100 Batch   60/156 - Loss:  0.234, Seconds: 8.64\n",
      "Epoch  23/100 Batch   80/156 - Loss:  0.217, Seconds: 7.48\n",
      "Epoch  23/100 Batch  100/156 - Loss:  0.239, Seconds: 9.32\n",
      "Average loss for this update: 0.231\n",
      "New Record!\n",
      "Epoch  23/100 Batch  120/156 - Loss:  0.234, Seconds: 10.11\n",
      "Epoch  23/100 Batch  140/156 - Loss:  0.237, Seconds: 10.27\n",
      "Average loss for this update: 0.234\n",
      "No Improvement.\n",
      "Epoch  24/100 Batch   20/156 - Loss:  0.270, Seconds: 8.12\n",
      "Epoch  24/100 Batch   40/156 - Loss:  0.249, Seconds: 9.55\n",
      "Average loss for this update: 0.258\n",
      "No Improvement.\n",
      "Epoch  24/100 Batch   60/156 - Loss:  0.253, Seconds: 8.54\n",
      "Epoch  24/100 Batch   80/156 - Loss:  0.219, Seconds: 7.44\n",
      "Epoch  24/100 Batch  100/156 - Loss:  0.251, Seconds: 9.32\n",
      "Average loss for this update: 0.239\n",
      "No Improvement.\n",
      "Epoch  24/100 Batch  120/156 - Loss:  0.246, Seconds: 10.15\n",
      "Epoch  24/100 Batch  140/156 - Loss:  0.229, Seconds: 10.07\n",
      "Average loss for this update: 0.235\n",
      "No Improvement.\n",
      "Epoch  25/100 Batch   20/156 - Loss:  0.286, Seconds: 8.22\n",
      "Epoch  25/100 Batch   40/156 - Loss:  0.233, Seconds: 7.98\n",
      "Average loss for this update: 0.256\n",
      "No Improvement.\n",
      "Epoch  25/100 Batch   60/156 - Loss:  0.246, Seconds: 8.56\n",
      "Epoch  25/100 Batch   80/156 - Loss:  0.214, Seconds: 7.46\n",
      "Epoch  25/100 Batch  100/156 - Loss:  0.230, Seconds: 9.26\n",
      "Average loss for this update: 0.228\n",
      "New Record!\n",
      "Epoch  25/100 Batch  120/156 - Loss:  0.218, Seconds: 10.11\n",
      "Epoch  25/100 Batch  140/156 - Loss:  0.203, Seconds: 10.17\n",
      "Average loss for this update: 0.212\n",
      "New Record!\n",
      "Epoch  26/100 Batch   20/156 - Loss:  0.254, Seconds: 7.60\n",
      "Epoch  26/100 Batch   40/156 - Loss:  0.237, Seconds: 7.86\n",
      "Average loss for this update: 0.237\n",
      "No Improvement.\n",
      "Epoch  26/100 Batch   60/156 - Loss:  0.213, Seconds: 8.60\n",
      "Epoch  26/100 Batch   80/156 - Loss:  0.174, Seconds: 7.52\n",
      "Epoch  26/100 Batch  100/156 - Loss:  0.208, Seconds: 11.41\n",
      "Average loss for this update: 0.196\n",
      "New Record!\n",
      "Epoch  26/100 Batch  120/156 - Loss:  0.192, Seconds: 11.31\n",
      "Epoch  26/100 Batch  140/156 - Loss:  0.189, Seconds: 10.13\n",
      "Average loss for this update: 0.192\n",
      "New Record!\n",
      "Epoch  27/100 Batch   20/156 - Loss:  0.232, Seconds: 7.70\n",
      "Epoch  27/100 Batch   40/156 - Loss:  0.192, Seconds: 7.88\n",
      "Average loss for this update: 0.206\n",
      "No Improvement.\n",
      "Epoch  27/100 Batch   60/156 - Loss:  0.199, Seconds: 8.90\n",
      "Epoch  27/100 Batch   80/156 - Loss:  0.177, Seconds: 7.51\n",
      "Epoch  27/100 Batch  100/156 - Loss:  0.205, Seconds: 9.76\n",
      "Average loss for this update: 0.197\n",
      "No Improvement.\n",
      "Epoch  27/100 Batch  120/156 - Loss:  0.188, Seconds: 10.61\n",
      "Epoch  27/100 Batch  140/156 - Loss:  0.189, Seconds: 10.11\n",
      "Average loss for this update: 0.19\n",
      "New Record!\n",
      "Epoch  28/100 Batch   20/156 - Loss:  0.217, Seconds: 8.68\n",
      "Epoch  28/100 Batch   40/156 - Loss:  0.196, Seconds: 7.84\n",
      "Average loss for this update: 0.204\n",
      "No Improvement.\n",
      "Epoch  28/100 Batch   60/156 - Loss:  0.202, Seconds: 8.52\n",
      "Epoch  28/100 Batch   80/156 - Loss:  0.184, Seconds: 7.49\n",
      "Epoch  28/100 Batch  100/156 - Loss:  0.202, Seconds: 9.32\n",
      "Average loss for this update: 0.197\n",
      "No Improvement.\n",
      "Epoch  28/100 Batch  120/156 - Loss:  0.195, Seconds: 10.13\n",
      "Epoch  28/100 Batch  140/156 - Loss:  0.178, Seconds: 10.73\n",
      "Average loss for this update: 0.186\n",
      "New Record!\n",
      "Epoch  29/100 Batch   20/156 - Loss:  0.215, Seconds: 8.36\n",
      "Epoch  29/100 Batch   40/156 - Loss:  0.197, Seconds: 7.90\n",
      "Average loss for this update: 0.205\n",
      "No Improvement.\n",
      "Epoch  29/100 Batch   60/156 - Loss:  0.194, Seconds: 8.59\n",
      "Epoch  29/100 Batch   80/156 - Loss:  0.182, Seconds: 7.43\n",
      "Epoch  29/100 Batch  100/156 - Loss:  0.196, Seconds: 9.48\n",
      "Average loss for this update: 0.188\n",
      "No Improvement.\n",
      "Epoch  29/100 Batch  120/156 - Loss:  0.178, Seconds: 10.15\n",
      "Epoch  29/100 Batch  140/156 - Loss:  0.172, Seconds: 10.13\n",
      "Average loss for this update: 0.176\n",
      "New Record!\n",
      "Epoch  30/100 Batch   20/156 - Loss:  0.245, Seconds: 7.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/100 Batch   40/156 - Loss:  0.203, Seconds: 7.90\n",
      "Average loss for this update: 0.216\n",
      "No Improvement.\n",
      "Epoch  30/100 Batch   60/156 - Loss:  0.198, Seconds: 8.59\n",
      "Epoch  30/100 Batch   80/156 - Loss:  0.172, Seconds: 7.59\n",
      "Epoch  30/100 Batch  100/156 - Loss:  0.195, Seconds: 9.35\n",
      "Average loss for this update: 0.191\n",
      "No Improvement.\n",
      "Epoch  30/100 Batch  120/156 - Loss:  0.197, Seconds: 10.31\n",
      "Epoch  30/100 Batch  140/156 - Loss:  0.183, Seconds: 10.13\n",
      "Average loss for this update: 0.184\n",
      "No Improvement.\n",
      "Epoch  31/100 Batch   20/156 - Loss:  0.216, Seconds: 7.97\n",
      "Epoch  31/100 Batch   40/156 - Loss:  0.201, Seconds: 7.89\n",
      "Average loss for this update: 0.206\n",
      "No Improvement.\n",
      "Epoch  31/100 Batch   60/156 - Loss:  0.192, Seconds: 8.67\n",
      "Epoch  31/100 Batch   80/156 - Loss:  0.167, Seconds: 7.52\n",
      "Epoch  31/100 Batch  100/156 - Loss:  0.178, Seconds: 9.43\n",
      "Average loss for this update: 0.175\n",
      "New Record!\n",
      "Epoch  31/100 Batch  120/156 - Loss:  0.172, Seconds: 10.21\n",
      "Epoch  31/100 Batch  140/156 - Loss:  0.167, Seconds: 10.17\n",
      "Average loss for this update: 0.168\n",
      "New Record!\n",
      "Epoch  32/100 Batch   20/156 - Loss:  0.213, Seconds: 7.88\n",
      "Epoch  32/100 Batch   40/156 - Loss:  0.209, Seconds: 7.86\n",
      "Average loss for this update: 0.203\n",
      "No Improvement.\n",
      "Epoch  32/100 Batch   60/156 - Loss:  0.187, Seconds: 9.05\n",
      "Epoch  32/100 Batch   80/156 - Loss:  0.163, Seconds: 9.69\n",
      "Epoch  32/100 Batch  100/156 - Loss:  0.190, Seconds: 9.96\n",
      "Average loss for this update: 0.18\n",
      "No Improvement.\n",
      "Epoch  32/100 Batch  120/156 - Loss:  0.171, Seconds: 10.46\n",
      "Epoch  32/100 Batch  140/156 - Loss:  0.189, Seconds: 10.21\n",
      "Average loss for this update: 0.179\n",
      "No Improvement.\n",
      "Epoch  33/100 Batch   20/156 - Loss:  0.228, Seconds: 7.68\n",
      "Epoch  33/100 Batch   40/156 - Loss:  0.213, Seconds: 8.86\n",
      "Average loss for this update: 0.216\n",
      "No Improvement.\n",
      "Epoch  33/100 Batch   60/156 - Loss:  0.199, Seconds: 8.60\n",
      "Epoch  33/100 Batch   80/156 - Loss:  0.175, Seconds: 7.82\n",
      "Epoch  33/100 Batch  100/156 - Loss:  0.205, Seconds: 10.01\n",
      "Average loss for this update: 0.189\n",
      "No Improvement.\n",
      "Epoch  33/100 Batch  120/156 - Loss:  0.174, Seconds: 11.17\n",
      "Epoch  33/100 Batch  140/156 - Loss:  0.179, Seconds: 10.86\n",
      "Average loss for this update: 0.18\n",
      "No Improvement.\n",
      "Epoch  34/100 Batch   20/156 - Loss:  0.221, Seconds: 7.97\n",
      "Epoch  34/100 Batch   40/156 - Loss:  0.194, Seconds: 8.02\n",
      "Average loss for this update: 0.2\n",
      "No Improvement.\n",
      "Epoch  34/100 Batch   60/156 - Loss:  0.183, Seconds: 10.04\n",
      "Epoch  34/100 Batch   80/156 - Loss:  0.162, Seconds: 7.53\n",
      "Epoch  34/100 Batch  100/156 - Loss:  0.180, Seconds: 11.21\n",
      "Average loss for this update: 0.176\n",
      "No Improvement.\n",
      "Epoch  34/100 Batch  120/156 - Loss:  0.176, Seconds: 10.19\n",
      "Epoch  34/100 Batch  140/156 - Loss:  0.186, Seconds: 10.21\n",
      "Average loss for this update: 0.18\n",
      "No Improvement.\n",
      "Epoch  35/100 Batch   20/156 - Loss:  0.212, Seconds: 7.84\n",
      "Epoch  35/100 Batch   40/156 - Loss:  0.182, Seconds: 8.06\n",
      "Average loss for this update: 0.192\n",
      "No Improvement.\n",
      "Epoch  35/100 Batch   60/156 - Loss:  0.184, Seconds: 8.81\n",
      "Epoch  35/100 Batch   80/156 - Loss:  0.160, Seconds: 8.69\n",
      "Epoch  35/100 Batch  100/156 - Loss:  0.170, Seconds: 12.00\n",
      "Average loss for this update: 0.171\n",
      "No Improvement.\n",
      "Epoch  35/100 Batch  120/156 - Loss:  0.161, Seconds: 11.24\n",
      "Epoch  35/100 Batch  140/156 - Loss:  0.173, Seconds: 10.94\n",
      "Average loss for this update: 0.163\n",
      "New Record!\n",
      "Epoch  36/100 Batch   20/156 - Loss:  0.189, Seconds: 7.78\n",
      "Epoch  36/100 Batch   40/156 - Loss:  0.152, Seconds: 7.96\n",
      "Average loss for this update: 0.165\n",
      "No Improvement.\n",
      "Epoch  36/100 Batch   60/156 - Loss:  0.147, Seconds: 8.74\n",
      "Epoch  36/100 Batch   80/156 - Loss:  0.130, Seconds: 7.61\n",
      "Epoch  36/100 Batch  100/156 - Loss:  0.153, Seconds: 9.65\n",
      "Average loss for this update: 0.143\n",
      "New Record!\n",
      "Epoch  36/100 Batch  120/156 - Loss:  0.143, Seconds: 10.67\n",
      "Epoch  36/100 Batch  140/156 - Loss:  0.138, Seconds: 12.18\n",
      "Average loss for this update: 0.14\n",
      "New Record!\n",
      "Epoch  37/100 Batch   20/156 - Loss:  0.162, Seconds: 7.86\n",
      "Epoch  37/100 Batch   40/156 - Loss:  0.150, Seconds: 8.84\n",
      "Average loss for this update: 0.158\n",
      "No Improvement.\n",
      "Epoch  37/100 Batch   60/156 - Loss:  0.153, Seconds: 8.98\n",
      "Epoch  37/100 Batch   80/156 - Loss:  0.130, Seconds: 7.46\n",
      "Epoch  37/100 Batch  100/156 - Loss:  0.150, Seconds: 10.35\n",
      "Average loss for this update: 0.14\n",
      "No Improvement.\n",
      "Epoch  37/100 Batch  120/156 - Loss:  0.132, Seconds: 11.14\n",
      "Epoch  37/100 Batch  140/156 - Loss:  0.133, Seconds: 12.51\n",
      "Average loss for this update: 0.13\n",
      "New Record!\n",
      "Epoch  38/100 Batch   20/156 - Loss:  0.160, Seconds: 7.74\n",
      "Epoch  38/100 Batch   40/156 - Loss:  0.151, Seconds: 7.91\n",
      "Average loss for this update: 0.155\n",
      "No Improvement.\n",
      "Epoch  38/100 Batch   60/156 - Loss:  0.153, Seconds: 8.75\n",
      "Epoch  38/100 Batch   80/156 - Loss:  0.138, Seconds: 7.61\n",
      "Epoch  38/100 Batch  100/156 - Loss:  0.143, Seconds: 9.59\n",
      "Average loss for this update: 0.143\n",
      "No Improvement.\n",
      "Epoch  38/100 Batch  120/156 - Loss:  0.156, Seconds: 10.21\n",
      "Epoch  38/100 Batch  140/156 - Loss:  0.150, Seconds: 10.20\n",
      "Average loss for this update: 0.15\n",
      "No Improvement.\n",
      "Epoch  39/100 Batch   20/156 - Loss:  0.186, Seconds: 7.84\n",
      "Epoch  39/100 Batch   40/156 - Loss:  0.175, Seconds: 7.93\n",
      "Average loss for this update: 0.177\n",
      "No Improvement.\n",
      "Epoch  39/100 Batch   60/156 - Loss:  0.157, Seconds: 8.73\n",
      "Epoch  39/100 Batch   80/156 - Loss:  0.162, Seconds: 7.49\n",
      "Epoch  39/100 Batch  100/156 - Loss:  0.178, Seconds: 9.64\n",
      "Average loss for this update: 0.167\n",
      "No Improvement.\n",
      "Epoch  39/100 Batch  120/156 - Loss:  0.176, Seconds: 10.23\n",
      "Epoch  39/100 Batch  140/156 - Loss:  0.167, Seconds: 10.32\n",
      "Average loss for this update: 0.169\n",
      "No Improvement.\n",
      "Epoch  40/100 Batch   20/156 - Loss:  0.214, Seconds: 7.79\n",
      "Epoch  40/100 Batch   40/156 - Loss:  0.173, Seconds: 8.00\n",
      "Average loss for this update: 0.19\n",
      "No Improvement.\n",
      "Epoch  40/100 Batch   60/156 - Loss:  0.176, Seconds: 8.71\n",
      "Epoch  40/100 Batch   80/156 - Loss:  0.146, Seconds: 7.51\n",
      "Epoch  40/100 Batch  100/156 - Loss:  0.171, Seconds: 9.56\n",
      "Average loss for this update: 0.161\n",
      "No Improvement.\n",
      "Epoch  40/100 Batch  120/156 - Loss:  0.165, Seconds: 10.37\n",
      "Epoch  40/100 Batch  140/156 - Loss:  0.141, Seconds: 10.45\n",
      "Average loss for this update: 0.156\n",
      "No Improvement.\n",
      "Epoch  41/100 Batch   20/156 - Loss:  0.200, Seconds: 7.91\n",
      "Epoch  41/100 Batch   40/156 - Loss:  0.162, Seconds: 7.95\n",
      "Average loss for this update: 0.178\n",
      "No Improvement.\n",
      "Epoch  41/100 Batch   60/156 - Loss:  0.168, Seconds: 8.67\n",
      "Epoch  41/100 Batch   80/156 - Loss:  0.142, Seconds: 7.98\n",
      "Epoch  41/100 Batch  100/156 - Loss:  0.155, Seconds: 9.67\n",
      "Average loss for this update: 0.151\n",
      "No Improvement.\n",
      "Epoch  41/100 Batch  120/156 - Loss:  0.159, Seconds: 10.30\n",
      "Epoch  41/100 Batch  140/156 - Loss:  0.138, Seconds: 10.32\n",
      "Average loss for this update: 0.15\n",
      "No Improvement.\n",
      "Epoch  42/100 Batch   20/156 - Loss:  0.208, Seconds: 7.92\n",
      "Epoch  42/100 Batch   40/156 - Loss:  0.162, Seconds: 7.99\n",
      "Average loss for this update: 0.18\n",
      "No Improvement.\n",
      "Epoch  42/100 Batch   60/156 - Loss:  0.163, Seconds: 8.77\n",
      "Epoch  42/100 Batch   80/156 - Loss:  0.144, Seconds: 7.54\n",
      "Epoch  42/100 Batch  100/156 - Loss:  0.154, Seconds: 9.41\n",
      "Average loss for this update: 0.153\n",
      "No Improvement.\n",
      "Epoch  42/100 Batch  120/156 - Loss:  0.147, Seconds: 10.36\n",
      "Epoch  42/100 Batch  140/156 - Loss:  0.144, Seconds: 10.30\n",
      "Average loss for this update: 0.144\n",
      "No Improvement.\n",
      "Epoch  43/100 Batch   20/156 - Loss:  0.187, Seconds: 7.78\n",
      "Epoch  43/100 Batch   40/156 - Loss:  0.148, Seconds: 7.96\n",
      "Average loss for this update: 0.162\n",
      "No Improvement.\n",
      "Epoch  43/100 Batch   60/156 - Loss:  0.144, Seconds: 8.85\n",
      "Epoch  43/100 Batch   80/156 - Loss:  0.148, Seconds: 7.53\n",
      "Epoch  43/100 Batch  100/156 - Loss:  0.140, Seconds: 9.96\n",
      "Average loss for this update: 0.144\n",
      "No Improvement.\n",
      "Epoch  43/100 Batch  120/156 - Loss:  0.147, Seconds: 11.73\n",
      "Epoch  43/100 Batch  140/156 - Loss:  0.144, Seconds: 10.86\n",
      "Average loss for this update: 0.147\n",
      "No Improvement.\n",
      "Epoch  44/100 Batch   20/156 - Loss:  0.172, Seconds: 8.46\n",
      "Epoch  44/100 Batch   40/156 - Loss:  0.152, Seconds: 9.47\n",
      "Average loss for this update: 0.162\n",
      "No Improvement.\n",
      "Epoch  44/100 Batch   60/156 - Loss:  0.150, Seconds: 9.97\n",
      "Epoch  44/100 Batch   80/156 - Loss:  0.145, Seconds: 7.88\n",
      "Epoch  44/100 Batch  100/156 - Loss:  0.168, Seconds: 9.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for this update: 0.151\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 20 # If the update loss does not decrease in 20 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./output/model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./output/model.ckpt\n",
      "Original Text: these condiments are overpriced and terrible the classic is disgustingly sweet the spiced tastes like a bad spicy marinara sauce from a chain restaurant\n",
      "\n",
      "Summary\n",
      "  Response Words: delicious traditional italian\n"
     ]
    }
   ],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "#input_sentence = \"\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0,len(clean_texts))\n",
    "input_sentence = clean_texts[381]\n",
    "text = text_to_seq(clean_texts[381])\n",
    "\n",
    "checkpoint = \"./output/model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "print('\\nSummary')\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
